{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4_OPhpWHsrNS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (roc_auc_score, precision_recall_curve,\n",
        "                           average_precision_score, confusion_matrix,\n",
        "                           classification_report)\n",
        "from sklearn.inspection import permutation_importance\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QqHSBJH8vZat"
      },
      "outputs": [],
      "source": [
        "class AdvancedLeadScoringModel:\n",
        "    \"\"\"\n",
        "    Lead Scoring Model with interpretability features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.model = RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            class_weight='balanced',\n",
        "            random_state=42\n",
        "        )\n",
        "        self.feature_importance = None\n",
        "        self.threshold = 0.5\n",
        "        self.documentation = {}\n",
        "\n",
        "    def _safe_division(self, a, b):\n",
        "        \"\"\"Safely perform division handling zeros and infinities\"\"\"\n",
        "        return np.where(b != 0, a / b, 0)\n",
        "\n",
        "    def prepare_features(self, df):\n",
        "        \"\"\"\n",
        "        Prepare features for the model\n",
        "        \"\"\"\n",
        "        try:\n",
        "            features = pd.DataFrame()\n",
        "            feature_docs = []\n",
        "\n",
        "            # Base metrics\n",
        "            base_metrics = [\n",
        "                'Outbound Calls (last month)',\n",
        "                'Personalized Outbound Emails (last month)',\n",
        "                'Demo Meeting Set (last month)',\n",
        "                'Demo Meeting Completed (last month)'\n",
        "            ]\n",
        "\n",
        "            for metric in base_metrics:\n",
        "                features[metric] = df[metric]\n",
        "                feature_docs.append({\n",
        "                    'name': metric,\n",
        "                    'type': 'base_metric',\n",
        "                    'description': f'Raw count of {metric.split(\"(\")[0].strip()}'\n",
        "                })\n",
        "\n",
        "            # Activity ratios\n",
        "            ratios = {\n",
        "                'email_to_call_ratio': (\n",
        "                    df['Personalized Outbound Emails (last month)'],\n",
        "                    df['Outbound Calls (last month)']\n",
        "                ),\n",
        "                'contact_rate': (\n",
        "                    df['Calls with Correct Contact (last month)'],\n",
        "                    df['Outbound Calls (last month)']\n",
        "                ),\n",
        "                'demo_set_rate': (\n",
        "                    df['Demo Meeting Set (last month)'],\n",
        "                    df['Calls with Correct Contact (last month)']\n",
        "                ),\n",
        "                'demo_completion_rate': (\n",
        "                    df['Demo Meeting Completed (last month)'],\n",
        "                    df['Demo Meeting Set (last month)']\n",
        "                )\n",
        "            }\n",
        "\n",
        "            for name, (numerator, denominator) in ratios.items():\n",
        "                features[name] = self._safe_division(numerator, denominator)\n",
        "                feature_docs.append({\n",
        "                    'name': name,\n",
        "                    'type': 'ratio',\n",
        "                    'description': f'Ratio of {numerator.name} to {denominator.name}',\n",
        "                    'calculation': f'{numerator.name} / {denominator.name}'\n",
        "                })\n",
        "\n",
        "            # Month-over-month changes\n",
        "            metrics = ['Outbound Calls', 'Demo Meeting Set', 'Opportunity Created']\n",
        "            for metric in metrics:\n",
        "                current = f'{metric} (last month)'\n",
        "                previous = f'{metric} (month before last)'\n",
        "                change_name = f'{metric}_change'\n",
        "\n",
        "                change = self._safe_division(\n",
        "                    df[current] - df[previous],\n",
        "                    df[previous]\n",
        "                )\n",
        "                features[change_name] = np.clip(change, -1, 1)\n",
        "\n",
        "                feature_docs.append({\n",
        "                    'name': change_name,\n",
        "                    'type': 'trend',\n",
        "                    'description': f'Month-over-month change in {metric}',\n",
        "                    'calculation': f'({current} - {previous}) / {previous}'\n",
        "                })\n",
        "\n",
        "            # Store feature documentation\n",
        "            self.documentation['features'] = feature_docs\n",
        "\n",
        "            # Clean data\n",
        "            features = features.replace([np.inf, -np.inf], 0)\n",
        "            features = features.fillna(0)\n",
        "\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in preparing features: {str(e)}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def train_with_validation(self, df):\n",
        "        \"\"\"\n",
        "        Train the model with comprehensive validation\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Prepare features and target\n",
        "            X = self.prepare_features(df)\n",
        "            y = (df['Opportunity Created (last month)'] > 0).astype(int)\n",
        "\n",
        "            # Split data\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y, test_size=0.2, random_state=42, stratify=y\n",
        "            )\n",
        "\n",
        "            # Scale features\n",
        "            X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "            X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "            # Cross-validation with multiple metrics\n",
        "            cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "            cv_metrics = {}\n",
        "\n",
        "            # Calculate cross-validation scores for each metric separately\n",
        "            scoring_metrics = ['accuracy', 'roc_auc', 'precision', 'recall']\n",
        "            for metric in scoring_metrics:\n",
        "                scores = cross_val_score(\n",
        "                    self.model, X_train_scaled, y_train,\n",
        "                    cv=cv, scoring=metric\n",
        "                )\n",
        "                cv_metrics[metric] = {\n",
        "                    'mean': scores.mean(),\n",
        "                    'std': scores.std()\n",
        "                }\n",
        "\n",
        "            # Train final model\n",
        "            self.model.fit(X_train_scaled, y_train)\n",
        "\n",
        "            # Calculate feature importance\n",
        "            self.feature_importance = pd.DataFrame({\n",
        "                'feature': X.columns,\n",
        "                'importance': self.model.feature_importances_\n",
        "            }).sort_values('importance', ascending=False)\n",
        "\n",
        "            # Calculate permutation importance\n",
        "            perm_importance = permutation_importance(\n",
        "                self.model, X_test_scaled, y_test,\n",
        "                n_repeats=10, random_state=42\n",
        "            )\n",
        "\n",
        "            # Generate model documentation\n",
        "            model_doc = {\n",
        "                'model_type': 'RandomForestClassifier',\n",
        "                'parameters': self.model.get_params(),\n",
        "                'feature_count': len(X.columns),\n",
        "                'training_samples': len(X_train),\n",
        "                'test_samples': len(X_test),\n",
        "                'class_distribution': {\n",
        "                    'positive': y.sum(),\n",
        "                    'negative': len(y) - y.sum()\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Calculate detailed metrics\n",
        "            y_pred_proba = self.model.predict_proba(X_test_scaled)[:, 1]\n",
        "            y_pred = y_pred_proba > self.threshold\n",
        "\n",
        "            metrics = {\n",
        "                'cross_validation': cv_metrics,\n",
        "                'test_performance': {\n",
        "                    'classification_report': classification_report(\n",
        "                        y_test, y_pred, output_dict=True\n",
        "                    ),\n",
        "                    'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),\n",
        "                    'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
        "                },\n",
        "                'feature_importance': {\n",
        "                    'random_forest': self.feature_importance.to_dict('records'),\n",
        "                    'permutation': {\n",
        "                        'mean': perm_importance.importances_mean.tolist(),\n",
        "                        'std': perm_importance.importances_std.tolist()\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Store documentation\n",
        "            self.documentation.update({\n",
        "                'model': model_doc,\n",
        "                'metrics': metrics\n",
        "            })\n",
        "\n",
        "            return {\n",
        "                'model_doc': model_doc,\n",
        "                'metrics': metrics,\n",
        "                'feature_importance': self.feature_importance\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in training model: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def score_leads(self, df):\n",
        "        \"\"\"\n",
        "        Score new leads using the trained model\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Prepare features\n",
        "            X = self.prepare_features(df)\n",
        "            if X.empty:\n",
        "                raise ValueError(\"Failed to prepare features\")\n",
        "\n",
        "            # Scale features\n",
        "            X_scaled = self.scaler.transform(X)\n",
        "\n",
        "            # Get probability scores\n",
        "            scores = self.model.predict_proba(X_scaled)[:, 1]\n",
        "\n",
        "            # Create scoring summary\n",
        "            scoring_summary = pd.DataFrame({\n",
        "                'user_name': df['user_name'],\n",
        "                'lead_score': scores,\n",
        "                'probability': scores,\n",
        "                'is_likely_opportunity': scores > self.threshold\n",
        "            })\n",
        "\n",
        "            # Add score categories\n",
        "            scoring_summary['category'] = pd.qcut(\n",
        "                scores,\n",
        "                q=5,\n",
        "                labels=['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
        "            )\n",
        "\n",
        "            return scoring_summary\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in scoring leads: {str(e)}\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPbCybaYvd1d",
        "outputId": "8ef14e91-a4f1-42b1-f037-8d1a09b2a6ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Documentation:\n",
            "{'model_type': 'RandomForestClassifier', 'parameters': {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}, 'feature_count': 11, 'training_samples': 40, 'test_samples': 10, 'class_distribution': {'positive': 25, 'negative': 25}}\n",
            "\n",
            "Cross-validation Results:\n",
            "{'accuracy': {'mean': 0.925, 'std': 0.1}, 'roc_auc': {'mean': 0.9866666666666667, 'std': 0.02666666666666666}, 'precision': {'mean': 1.0, 'std': 0.0}, 'recall': {'mean': 0.9028571428571428, 'std': 0.1220237514517864}}\n",
            "\n",
            "Feature Importance:\n",
            "                               feature  importance\n",
            "3  Demo Meeting Completed (last month)    0.324152\n",
            "2        Demo Meeting Set (last month)    0.207849\n",
            "6                        demo_set_rate    0.096116\n",
            "0          Outbound Calls (last month)    0.080991\n",
            "5                         contact_rate    0.080742\n",
            "\n",
            "Lead Scoring Results:\n",
            "         user_name  lead_score  probability  is_likely_opportunity  category\n",
            "0      Emma Wilson        0.09         0.09                  False       Low\n",
            "1      Marcus Chen        0.18         0.18                  False    Medium\n",
            "2      Sarah Patel        0.04         0.04                  False  Very Low\n",
            "3  James Rodriguez        0.96         0.96                   True      High\n",
            "4       Aisha Khan        0.07         0.07                  False       Low\n"
          ]
        }
      ],
      "source": [
        "def main(df):\n",
        "    \"\"\"\n",
        "    Main function to run the lead scoring system\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize model\n",
        "        model = AdvancedLeadScoringModel()\n",
        "\n",
        "        # Train and validate model\n",
        "        training_results = model.train_with_validation(df)\n",
        "        if training_results is None:\n",
        "            raise ValueError(\"Model training failed\")\n",
        "\n",
        "        # Score leads\n",
        "        scoring_results = model.score_leads(df)\n",
        "        if scoring_results is None:\n",
        "            raise ValueError(\"Lead scoring failed\")\n",
        "\n",
        "        # Compile complete documentation\n",
        "        documentation = {\n",
        "            'model_documentation': model.documentation,\n",
        "            'training_results': training_results,\n",
        "            'scoring_results': scoring_results\n",
        "        }\n",
        "\n",
        "        return documentation\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main execution: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "\n",
        "  # Read data\n",
        "  df = pd.read_csv('monthly_report_expanded.csv')\n",
        "\n",
        "  # Run analysis\n",
        "  results = main(df)\n",
        "\n",
        "  if results is not None:\n",
        "      print(\"\\nModel Documentation:\")\n",
        "      print(results['model_documentation']['model'])\n",
        "\n",
        "      print(\"\\nCross-validation Results:\")\n",
        "      print(results['model_documentation']['metrics']['cross_validation'])\n",
        "\n",
        "      print(\"\\nFeature Importance:\")\n",
        "      print(results['training_results']['feature_importance'].head())\n",
        "\n",
        "      print(\"\\nLead Scoring Results:\")\n",
        "      print(results['scoring_results'].head())\n",
        "  else:\n",
        "      print(\"Analysis failed to complete\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
